{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AllenNLP demo**\n",
    "\n",
    "This is a demo for prediction venue based on title and abstract of the paper\n",
    "\n",
    "reference: https://github.com/allenai/allennlp-as-a-library-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Iterator, List, Dict, Optional\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for dataset reader\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "# read pretrained embedding from AWS S3\n",
    "from allennlp.modules.token_embedders.embedding import _read_embeddings_from_text_file\n",
    "\n",
    "# for building model\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create classes for the model**\n",
    "\n",
    "- `DatasetReader`: to read dataset and return `Instance` class\n",
    "- `Model`: input `Instance` class and return output prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PublicationDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for publication and venue dataaet\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self._tokenizer = tokenizer or WordTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        \"\"\"\n",
    "        Read publication and venue dataset in JSON format\n",
    "        \n",
    "        Data is in the following format:\n",
    "            {\"title\": ..., \"paperAbstract\": ..., \"venue\": ...}\n",
    "        \"\"\"\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            for line in data_file:\n",
    "                line = line.strip(\"\\n\")\n",
    "                if not line:\n",
    "                    continue\n",
    "                paper_json = json.loads(line)\n",
    "                title = paper_json['title']\n",
    "                abstract = paper_json['paperAbstract']\n",
    "                venue = paper_json['venue']\n",
    "                yield self.text_to_instance(title, abstract, venue)\n",
    "        \n",
    "    def text_to_instance(self, \n",
    "                         title: str, \n",
    "                         abstract: str, \n",
    "                         venue: str=None) -> Instance:\n",
    "        \"\"\"\n",
    "        Turn title, abstract, and venue to instance\n",
    "        \"\"\"\n",
    "        tokenized_title = self._tokenizer.tokenize(title)\n",
    "        tokenized_abstract = self._tokenizer.tokenize(abstract)\n",
    "        title_field = TextField(tokenized_title, self._token_indexers)\n",
    "        abstract_field = TextField(tokenized_abstract, self._token_indexers)\n",
    "        fields = {'title': title_field, \n",
    "                  'abstract': abstract_field}\n",
    "        if venue is not None:\n",
    "            fields['label'] = LabelField(venue)\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcademicPaperClassifier(Model):\n",
    "    \"\"\"\n",
    "    Model to classify venue based on input title and abstract\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 title_encoder: Seq2VecEncoder,\n",
    "                 abstract_encoder: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.title_encoder = title_encoder\n",
    "        self.abstract_encoder = abstract_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.metrics = {\n",
    "                \"accuracy\": CategoricalAccuracy(),\n",
    "                \"accuracy3\": CategoricalAccuracy(top_k=3)\n",
    "        }\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        initializer(self)\n",
    "    \n",
    "    def forward(self, \n",
    "                title: Dict[str, torch.LongTensor],\n",
    "                abstract: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        embedded_title = self.text_field_embedder(title)\n",
    "        title_mask = get_text_field_mask(title)\n",
    "        encoded_title = self.title_encoder(embedded_title, title_mask)\n",
    "\n",
    "        embedded_abstract = self.text_field_embedder(abstract)\n",
    "        abstract_mask = get_text_field_mask(abstract)\n",
    "        encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)\n",
    "\n",
    "        logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=-1))\n",
    "        class_probabilities = F.softmax(logits, dim=-1)\n",
    "        argmax_indices = np.argmax(class_probabilities.cpu().data.numpy(), axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\") for x in argmax_indices]\n",
    "        output_dict = {\n",
    "            'logits': logits, \n",
    "            'class_probabilities': class_probabilities,\n",
    "            'predicted_label': labels, \n",
    "            'labels': labels\n",
    "        }\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read dataset**\n",
    "\n",
    "- `cached_path`: can cache the file locally\n",
    "- `BasicTextFieldEmbedder` takes a mapping from index names to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/train.jsonl\"\n",
    "validation_data_path = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/dev.jsonl\"\n",
    "pretrained_file = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PublicationDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = reader.text_to_instance(\"This is a great paper.\", \n",
    "                                   \"Indeed, this is a great paper of all time\", \n",
    "                                   \"Nature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [01:14, 201.32it/s]\n",
      "2000it [00:10, 194.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = reader.read(cached_path(train_data_path))\n",
    "validation_dataset = reader.read(cached_path(validation_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [00:06<00:00, 2722.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# building vocabulary\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:05, 70517.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained embedding\n",
    "embedding_matrix = _read_embeddings_from_text_file(file_uri=pretrained_file, \n",
    "                                                   embedding_dim=100, \n",
    "                                                   vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64714, 100])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "num_classes = len(vocab.get_index_to_token_vocabulary('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), \n",
    "                            embedding_dim=EMBEDDING_DIM,\n",
    "                            weight=embedding_matrix)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_title = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n",
    "                                                 batch_first=True, bidirectional=True))\n",
    "lstm_abstract = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n",
    "                                                    batch_first=True, bidirectional=True))\n",
    "feed_forward = torch.nn.Linear(2 * 2 * HIDDEN_DIM, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AcademicPaperClassifier(vocab,\n",
    "                                word_embeddings, \n",
    "                                lstm_title, \n",
    "                                lstm_abstract, \n",
    "                                feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=64, \n",
    "                          sorting_keys=[(\"abstract\", \"num_tokens\"), \n",
    "                                        (\"title\", \"num_tokens\")])\n",
    "iterator.index_with(vocab) # index with the created vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    patience=2,\n",
    "    num_epochs=5,\n",
    "    serialization_dir='output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.0923 ||: 100%|██████████| 235/235 [08:19<00:00,  2.13s/it]\n",
      "loss: 1.0824 ||: 100%|██████████| 32/32 [00:05<00:00,  5.48it/s]\n",
      "loss: 1.0831 ||: 100%|██████████| 235/235 [08:28<00:00,  2.16s/it]\n",
      "loss: 1.0768 ||: 100%|██████████| 32/32 [00:05<00:00,  5.86it/s]\n",
      "loss: 1.0781 ||: 100%|██████████| 235/235 [09:46<00:00,  2.50s/it]\n",
      "loss: 1.0719 ||: 100%|██████████| 32/32 [00:05<00:00,  5.94it/s]\n",
      "loss: 1.0735 ||: 100%|██████████| 235/235 [07:57<00:00,  2.03s/it]\n",
      "loss: 1.0675 ||: 100%|██████████| 32/32 [00:04<00:00,  6.44it/s]\n",
      "loss: 1.0689 ||: 100%|██████████| 235/235 [08:09<00:00,  2.08s/it]\n",
      "loss: 1.0630 ||: 100%|██████████| 32/32 [00:05<00:00,  6.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training_duration': '00:43:08',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 4,\n",
       " 'epoch': 4,\n",
       " 'training_loss': 1.0689376772718227,\n",
       " 'validation_loss': 1.0630338974297047,\n",
       " 'best_epoch': 4,\n",
       " 'best_validation_loss': 1.0630338974297047}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prediction**\n",
    "\n",
    "Lastly, we can also write the prediction class `PaperClassifierPredictor` which take input any `json_dict` and return the `Instance`. The AllenNLP will take care of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.predictors.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperClassifierPredictor(Predictor):\n",
    "    \"\"\"\"\n",
    "    Predictor wrapper for the AcademicPaperClassifier\n",
    "    \"\"\"\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        title = json_dict['title']\n",
    "        abstract = json_dict['paperAbstract']\n",
    "        instance = self._dataset_reader.text_to_instance(title=title, abstract=abstract)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PaperClassifierPredictor(model, dataset_reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': [0.1362348347902298, 0.015435554087162018, -0.15664474666118622], 'class_probabilities': [0.3798924684524536, 0.33666518330574036, 0.28344231843948364], 'predicted_label': 'ACL', 'labels': 'ACL'}\n"
     ]
    }
   ],
   "source": [
    "prediction_output = predictor.predict_json(\n",
    "    {\n",
    "        \"title\": \"Know What You Don't Know: Unanswerable Questions for SQuAD\", \n",
    "        \"paperAbstract\": \"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load model for prediction**\n",
    "\n",
    "We can load trained model from `serialization_dir` that we specified and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/titipata/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from venue.venue_predictor import PaperClassifierPredictor\n",
    "from venue.venue_reader import PublicationDatasetReader\n",
    "from venue.venue_classifier import AcademicPaperClassifier\n",
    "\n",
    "archive = load_archive('output/model.tar.gz')\n",
    "venue_predictor = PaperClassifierPredictor.from_archive(archive, 'venue_predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': [1.4720406532287598, -0.062336090952157974, -1.505210518836975], 'class_probabilities': [0.7895634770393372, 0.17022199928760529, 0.04021456465125084], 'predicted_label': 'ACL', 'labels': 'ACL'}\n"
     ]
    }
   ],
   "source": [
    "prediction_output = venue_predictor.predict_json(\n",
    "    {\n",
    "        \"title\": \"Know What You Don't Know: Unanswerable Questions for SQuAD\", \n",
    "        \"paperAbstract\": \"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prediction_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
